{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75000 files belonging to 3 classes.\n",
      "Using 60000 files for training.\n",
      "Found 75000 files belonging to 3 classes.\n",
      "Using 15000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Number of batches in raw_train_ds: 1875\n",
      "Number of batches in raw_val_ds: 469\n",
      "Number of batches in raw_test_ds: 782\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=12354\n",
    ")\n",
    "\n",
    "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=12354\n",
    ")\n",
    "\n",
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/test\",\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"First, a warning. 'How to Marry a Millionaire' comes prefaced by an apparently random five minute orchestral performance of 'Street Scene', a Gershwin-lite piece treated with the full pomp and ceremony of, well, Gershwin. Sitting through it takes some patience. If you have the DVD, rest assured, you can skip forward. You won't miss anything.<br /><br />The film itself is one of the perpetual disappointments of 50's Hollywood, a movie so bolstered by major star-power, opulent mise-en-scene and perfect high-concept that failure seems inconceivable. The title alone is perfect. Generation after generation, however, are forced to ask themselves - how is this so limp? The script is an albatross about the production's neck, a dead, smelling thing that chokes everything and everyone before they can really spark to life. There are no comic situations, just isolated moments that play for laughs. Whenever an actual comedy scene threatens to develop, the movie quickly moves on to other, less interesting things. A case in point - the scene where the three leading ladies each bring a date to the same fancy restaurant. One of them, short-sighted, refuses to wear her spectacles out of vanity. One of the dates is married. A classic Hollywood farce set-up, surely, complete with mistaken identity, angry wife, and probably a pie in the face for somebody? Well, no. Instead, we cut between the three dates as the ladies react 'comically' to things their partners say. Hit the punchline, and cut to the next limp joke. If in doubt, have Marilyn walk into a wall. Where's Billy Wilder when you need him?<br /><br />The three stars are almost a perfect diagram of the life cycle of the classic Hollywood screen goddess. This was one of Marilyn Monroe's breakout films, and the camera just eats her up, even though the script gives her nothing to do. She's so luminescent she almost seems newly hatched. Lauren Bacall, on the other hand, had been a major star for nearly a full decade, and she knows how to dominate the screen even when in frame with Monroe. She gets the only thing passing for a real role, and delivers the few good lines with a cynical snap - given the right material, she could have brought this thing to life. She's a curiously ageless actress - when she lies about her age in the film and claims to be forty, it isn't instantly ridiculous - and far less girlish than her co-stars, giving her a convincing authority. Betty Grable was far from ageless, and had a good eight years on her co-stars, putting her near the end of her Hollywood career. There's an air of desperation about her at times, stranded on screen with nothing but a toothpaste smile and a few scraps of comic timing, unable to play her real age but fooling no-one as a contemporary of this new, sharper generation of actresses, relying on the same old schtick that had served her throughout her career (for Marilyn-doubters, seeing the two juxtaposed in this movie helps to throw Monroe's subtlety and - yes - intelligence into sharp relief). She's also lumbered with the dead wood in terms of male co-stars (although all of the men - even the great William Powell - are guilty of lazy performances); she's unable to strike any comic sparks off them. Better to have given her role to the under-utilised Monroe, who could be funny all by herself, and left Grable with the repetitive Mr. Magoo routine.<br /><br />That the movie is as enjoyable as it is can be put down to the luscious Hollywood production, the sort that renders even the twee likes of 'By the Light of the Silvery Moon' watchable. But somewhere, buried beneath the flabby jokes and professionalism, lies the rough outline of a sharp, cynical comedy about the business of marriage that Bacall could have made sing - and new generations of movie viewers will sit down with 'How to Marry a Millionaire' in expectation of that movie, ready to be disappointed all over again.\"\n",
      "0\n",
      "b'An interesting premise, and Billy Drago is always good as a dangerous nut-bag (side note: I\\'d love to see Drago, Stephen McHattie and Lance Hendrikson in a flick together; talk about raging cheekbones!). The soundtrack wasn\\'t terrible, either.<br /><br />But the acting--even that of such professionals as Drago and Debbie Rochon--was terrible, the directing worse (perhaps contributory to the former), the dialog chimp-like, and the camera work, barely tolerable. Still, it was the SETS that got a big \"10\" on my \"oy-vey\" scale. I don\\'t know where this was filmed, but were I to hazard a guess, it would be either an open-air museum, or one of those re-enactment villages, where everything is just a bit too well-kept to do more than suggest the \"real Old West\". Okay, so it was shot on a college kid\\'s budget. That said, I could have forgiven one or two of the aforementioned faults. But taken all together, and being generous, I could not see giving it more than three stars.'\n",
      "0\n",
      "b'Wow. I just saw Demon Wind a little while ago, and I don\\'t think I\\'ll ever be the same. It has the power to inspire nightmares, but for all the wrong reasons, actually.<br /><br />Never before has humanity seen such a gratuitous change in make-up, for no damn reason. Or, similarly, so much bad zombie (?) makeup that makes you hungry for those Halloween green marshmallows.<br /><br />Or so much naked old lady, for that matter. But then, there was \"The Shining.\"<br /><br />The plot here is so amateurish that it actually almost holds a little bit of charm, as does the dialog. The last shot of the film is just so silly that its beyond description. It\\'s like some drunk college student got together with some pals and decided to throw Bruce Willis type dialog together with (I guess?) teenybopper dialog from some Elm Street film. The result is jarring, and it\\'d be truly funny if it was intended that way.<br /><br />Ah, what the hey. I\\'ll laugh anyway.<br /><br />Hell, get together with your friends and watch this. But make absolutely sure you\\'re drunk first. Or, you may go insane. Particularly if you\\'re a college film student.<br /><br />Cheers.'\n",
      "0\n",
      "b'this has to be one of the best and most useful shows on TV. keys to the v.i.p. demonstrates some of the best seduction techniques and the humor that goes along with the techniques that are not up to par. to the person who wrote the negative comment, i only have one thing to say. stop hating on us because we are better looking and have more game then you. have you ever seen the inside of a club or do you just watch it on TV. and your so called female friend. she is not attracted to us because if guys like me saw her in the club, we would just walk right by and talk to the hot girls, like the ones on the show.<br /><br />STOP HATING watch keys to the V.I.P. and improve your game'\n",
      "1\n",
      "b\"This is not a serious film, and does not pretend to be, but it is not as bad as some of its reviews, it's title, and the first ten minutes lead you to expect.<br /><br />The plot is very silly, but this adds to the light-hearted fun and enthusiasm which runs through the film. The characters are played sympathetically, and while they do engage in typical teenage angst, they generally avoid the sickly sentimentality usually to be found in this film genre.<br /><br />Unusually set in London, sympathetic to geeks, this is well worth a watch if it happens to be on; if you want some tongue-in-cheek silliness, and don't mind suspending your disbelief.\"\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(5):\n",
    "        print(text_batch.numpy()[i])\n",
    "        print(label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import layers\n",
    "import string \n",
    "import re # regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation\n",
    "\n",
    "# html tags and punctuation removal from text \n",
    "def custom_standardization(input_data):  \n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model constants.\n",
    "max_features = 20000 # Number of words to use.\n",
    "embedding_dim = 128 # Dimension of the embedding vector.\n",
    "sequence_length = 500 # cut off the text after 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Vectorize the text samples into a 2D integer tensor of shape [batch_size, sequence_length]\n",
    "# vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "#     standardize=custom_standardization,\n",
    "#     max_tokens=max_features,\n",
    "#     output_mode=\"int\",\n",
    "#     output_sequence_length=sequence_length\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label): # vectorize the text and label into a 2D integer tensor of shape [batch_size, sequence_length]\n",
    "    text = tf.expand_dims(text, -1) # Add a dimension to the end of the tensor.\n",
    "    return vectorize_layer(text), label # Return the vectorized text and the label.\n",
    "\n",
    "# Vectorize the data.\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "# Do async prefetching / buffering of the data for best performance on GPU.\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# A integer input for vocab indices.\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "# 'embedding_dim'.\n",
    "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with the Adam optimizer and binary crossentropy loss.\n",
    "# Adam is a gradient-based optimizer that is good for stochastic gradient descent.\n",
    "# Binary crossentropy is a loss function that can be used to train a binary classifier.\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "# Fit the model using the train and test datasets.\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\Code\\AI-Machine-Learning\\Keras\\Natural Languag Processing\\TextProcessing.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Code/AI-Machine-Learning/Keras/Natural%20Languag%20Processing/TextProcessing.ipynb#ch0000015?line=3'>4</a>\u001b[0m indices \u001b[39m=\u001b[39m vectorize_layer(inputs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Code/AI-Machine-Learning/Keras/Natural%20Languag%20Processing/TextProcessing.ipynb#ch0000015?line=4'>5</a>\u001b[0m \u001b[39m# Turn vocab indices into predictions\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Code/AI-Machine-Learning/Keras/Natural%20Languag%20Processing/TextProcessing.ipynb#ch0000015?line=5'>6</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(indices)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# A string input\n",
    "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
    "# Turn strings into vocab indices\n",
    "indices = vectorize_layer(inputs)\n",
    "# Turn vocab indices into predictions\n",
    "outputs = model(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our end to end model\n",
    "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
    "end_to_end_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "end_to_end_model.evaluate(raw_test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "acbe944587e9153b3760fe6bdea35fbf77e65f6dcc869406197e5f964786d5ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
